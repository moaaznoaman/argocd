---
# prometheus: {}
vaultUrl: "https://env-suite.vault.azure.net/"
envDomain: "grafana.cluster.local"
# For the external secrets operator to pull secrets
suite: cbtest

prometheusSpec:
  admissionWebhooks:
    patch:
      enabled: false
      
promtail:
  enabled: false
  config:
    logLevel: debug
    clients:
      - url: http://prometheus-loki-gateway.monitoring.svc.cluster.local/loki/api/v1/push
#    extraVolumeMounts:
#      - mountPath: /var/log/journal
#        name: journal
#        readOnly: true
#    extraVolumes:
#      - hostPath:
#          path: /var/log/journal
#        name: journal
    # server:
    #   grpc_server_max_recv_msg_size: 8388608
    #   grpc_server_max_send_msg_size: 8388608
    snippets:
      pipeline_stages:
        - cri: {}
        - json:
            expressions:
              level: level
              request_id: request_id
              time: time
        - labels:
            level:
#      scrapeConfigs: |
#        - job_name: kubernetes-pods
#          static_configs:
#            - targets:
#                - localhost
#              labels:
#                job: kubernetes-pods
#                __path__: /var/log/pods/*/*/*.log
#          relabel_configs:
#            - source_labels: [__path__]
#              regex: '.*/var/log/pods/([^_]+)_([^_]+)_[^/]+/.*\.log'
#              replacement: '$1'
#              target_label: namespace
#            - source_labels: [__path__]
#              regex: '.*/var/log/pods/([^_]+)_([^_]+)_[^/]+/.*\.log'
#              replacement: '$2'
#              target_label: pod
#            - source_labels: [__path__]
#              regex: '.*/var/log/pods/[^_]+_[^_]+_[^/]+/([^_]+)-.*\.log'
#              replacement: '$1'
#              target_label: container
      extraRelabelConfigs:
        - action: labeldrop
          regex: "(app|container|instance|component|namespace)"
#        - action: replace
#          replacement: default
#          target_label: suite

kube-prometheus-stack:
  enabled: false
  namespaceOverride: "monitoring"
  # fullnameOverride: "demo-services"
  alertmanager:
    enabled: false
    alertmanagerSpec:
      logLevel: info
      secrets:
        - alertmanager-secrets
    config:
      global:
        resolve_timeout: 5m
      receivers:
        - name: "null"
        - name: filia_monitoring_slack
          slack_configs:
            - api_url_file: /etc/alertmanager/secrets/alertmanager-secrets/filia-monitoring-slack-url
              channel: "#filia-monitoring"
              send_resolved: true
              text: |-
                {{ range .Alerts }}
                  *Alert:* {{ .Annotations.summary }} - `{{ .Labels.environment }}`
                  *Description:* {{ .Annotations.description }} - `{{ .Labels.severity }}`
                  *Details:*
                  {{ range .Labels.SortedPairs }} â€¢ *{{ .Name }}:* `{{ .Value }}`
                  {{ end }}
                {{ end }}
              title:
                '{{ if eq .Status "firing" }}:fire:{{ else }}:white_check_mark:{{ end
                }}[{{ .Status | toUpper }}{{ if eq .Status "firing" }}{{ .Alerts.Firing
                | len }}{{ end }}] Monitoring Event'
        - name: ops_monitoring_slack
          slack_configs:
            - api_url_file: /etc/alertmanager/secrets/alertmanager-secrets/ops-monitoring-slack-url
              channel: "#ops-monitoring"
              send_resolved: true
              text: |-
                {{ range .Alerts }}
                  *Alert:* {{ .Annotations.summary }} - `{{ .Labels.environment }}`
                  *Description:* {{ .Annotations.description }} - `{{ .Labels.severity }}`
                  *Details:*
                  {{ range .Labels.SortedPairs }}{{ if or (eq .Name "alertname") (eq .Name "cluster") (eq .Name "environment") (eq .Name "severity") (eq .Name "instance") (eq .Name "target") (eq .Name "team") (eq .Name "dashboard") }}*{{ .Name }}*: `{{ .Value }}`
                  {{ end }}{{ end }}
                {{ end }}
              title:
                '{{ if eq .Status "firing" }}:fire:{{ else }}:white_check_mark:{{ end
                }}[{{ .Status | toUpper }}{{ if eq .Status "firing" }}{{ .Alerts.Firing
                | len }}{{ end }}] Monitoring Event'
      route:
        group_by:
          - cluster
          - alertname
        group_interval: 30s
        group_wait: 10s
        receiver: "null"
        repeat_interval: 24h
        routes:
          - match:
              team: development
            receiver: filia_monitoring_slack
          - match:
              team: ops
            receiver: ops_monitoring_slack
  grafana:
    envFromSecret: "loki-new-basic-auth"
    enabled: true
    admin:
      existingSecret: "grafana-secret-creds"
    # dashboardProviders:
    #   dashboardproviders.yaml:
    #     apiVersion: 1
    #     providers:
    #       - disableDeletion: false
    #         editable: true
    #         folder: ""
    #         name: filia-dashboards
    #         options:
    #           path: /var/lib/grafana/dashboards/filia-dashboards
    #         orgId: 1
    #         type: file
    # dashboardsConfigMaps:
    #   filia-dashboards: filia-dashboards-configmap
    datasources:
      datasources.yaml:
        apiVersion: 1
        datasources:
          - access: proxy
            isDefault: true
            name: prometheus
            type: prometheus
            url: http://prometheus-kube-prometheus-prometheus.monitoring.svc:9090
        apiVersion: 1
        datasources:
          - name: Loki
            type: loki
            url: 'http://prometheus-loki-gateway.monitoring.svc.cluster.local'
            access: proxy
            basicAuth: true
            basicAuthUser: loki
            secureJsonData:
              basicAuthPassword: $__env{password}
    grafana.ini:
      auth.anonymous:
        enabled: false
        org_name: Main Org.
        org_role: Viewer
      auth.basic:
        enabled: true
      # dashboards:
      #   default_home_dashboard_path: /var/lib/grafana/dashboards/filia-dashboards/core-system.json
    podAnnotations:
      reloader.stakater.com/auto: "true"
    sidecar:
      dashboards:
        multicluster:
          global:
            enabled: true
      datasources:
        defaultDatasourceEnabled: false
    # ingress:
    #   enabled: true
    #   ingressClassName: public
    #   annotations: {}
    #   hosts:
    #     - xxx
    #   tls:
    #     - secretName: monitoring-tls-secret
    #       hosts:
    #         - xxx
  kubeControllerManager:
    enabled: false
  kubeScheduler:
    enabled: false
  prometheus:
    enabled: false
    service:
      type: ClusterIP
      port: 9090
      targetPort: 9090
      annotations:
        service.beta.kubernetes.io/azure-load-balancer-internal: "true"
        service.beta.kubernetes.io/azure-load-balancer-internal-subnet: "ingress-internal"
    prometheusSpec:
      admissionWebhooks:
        patch:
          enabled: false
      portName: http
      resources:
        limits:
          memory: "2500Mi"
        requests:
          memory: "400Mi"
      externalLabels:
        cluster: "default"
      ruleSelector:
        matchLabels: {}
      serviceMonitorSelector: {}
      storageSpec:
        volumeClaimTemplate:
          spec:
            accessModes: ["ReadWriteOnce"]
            resources:
              requests:
                storage: "16Gi"
  prometheus-node-exporter:
    {}
    # resources:
    #   limits:
    #     cpu: 200m
    #     memory: 100Mi
  prometheusOperator:
    resources:
      limits:
        cpu: 200m
        memory: 200Mi
      requests:
        cpu: 100m
        memory: 100Mi

loki:
  singleBinary:
    replicas: 1
    persistence:
      enabled: true
      storageClassName: standard  # or "local-path" or any StorageClass in your cluster
      accessModes:
        - ReadWriteOnce
      size: 10Gi
  deploymentMode: SingleBinary
  lokiCanary:
    enabled: false
  test:
    enabled: false
  backend:
    enabled: false
    replicas: 0
  read:
    enabled: false
    replicas: 0
  write:
    enabled: false
    replicas: 0

  loki:
    auth_enabled: false
    commonConfig:
      replication_factor: 1
      ring:
        kvstore:
          store: memberlist

    compactor:
      retention_enabled: false
#      compaction_interval: 10m
#      retention_delete_delay: 2h
#      retention_delete_worker_count: 150
#      working_directory: /var/loki/data/retention

    ingester:
      chunk_encoding: snappy

    pattern_ingester:
      enabled: true

    limits_config:
      allow_structured_metadata: true
      volume_enabled: true
      retention_period: 672h

    schemaConfig:
      configs:
        - from: "2023-07-01"
          store: boltdb-shipper
          object_store: filesystem
          schema: v12
          index:
            period: 24h
            prefix: index_
          chunks:
            prefix: chunk_
            period: 24h
        - from: "2024-05-07"
          store: tsdb
          object_store: filesystem
          schema: v13
          index:
            period: 24h
            prefix: index_

    storage_config:
      tsdb_shipper:
        active_index_directory: /var/loki/data/loki/index
        cache_location: /var/loki/data/loki/boltdb-cache
        cache_ttl: "24h"
      filesystem:
        directory: /var/loki/data/loki/chunks

    storage:
      type: filesystem
      filesystem:
        chunks_directory: /var/loki/data/loki/chunks

    podSecurityContext:
      fsGroup: 10001
      runAsGroup: 10001
      runAsNonRoot: true
      runAsUser: 10001

    monitoring:
      serviceMonitor:
        enabled: true
      selfMonitoring:
        enabled: false
        grafanaAgent:
          installOperator: false
      lokiCanary:
        enabled: false

    rbac:
      pspEnabled: false

    querier:
      max_concurrent: 4

  tableManager:
    retention_deletes_enabled: "false"
    retention_period: "336h"

  gateway:
    basicAuth:
      enabled: true
      existingSecret: loki-new-basic-auth
#  serviceAccount:
#    name: loki # The app ID of the Azure AD app
#    labels:
#      "azure.workload.identity/use": "true"
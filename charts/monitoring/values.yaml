---
# prometheus: {}
vaultUrl: "https://env-suite.vault.azure.net/"
envDomain: "grafana.cluster.local"
# For the external secrets operator to pull secrets
suite: cbtest

prometheusSpec:
  admissionWebhooks:
    patch:
      enabled: false
      
promtail:
  enabled: false
  config:
    logLevel: debug
    clients:
      - url: http://prometheus-loki-gateway.monitoring.svc.cluster.local/loki/api/v1/push
#    extraVolumeMounts:
#      - mountPath: /var/log/journal
#        name: journal
#        readOnly: true
#    extraVolumes:
#      - hostPath:
#          path: /var/log/journal
#        name: journal
    # server:
    #   grpc_server_max_recv_msg_size: 8388608
    #   grpc_server_max_send_msg_size: 8388608
    snippets:
      pipeline_stages:
        - cri: {}
        - json:
            expressions:
              level: level
              request_id: request_id
              time: time
        - labels:
            level:
#      scrapeConfigs: |
#        - job_name: kubernetes-pods
#          static_configs:
#            - targets:
#                - localhost
#              labels:
#                job: kubernetes-pods
#                __path__: /var/log/pods/*/*/*.log
#          relabel_configs:
#            - source_labels: [__path__]
#              regex: '.*/var/log/pods/([^_]+)_([^_]+)_[^/]+/.*\.log'
#              replacement: '$1'
#              target_label: namespace
#            - source_labels: [__path__]
#              regex: '.*/var/log/pods/([^_]+)_([^_]+)_[^/]+/.*\.log'
#              replacement: '$2'
#              target_label: pod
#            - source_labels: [__path__]
#              regex: '.*/var/log/pods/[^_]+_[^_]+_[^/]+/([^_]+)-.*\.log'
#              replacement: '$1'
#              target_label: container
      extraRelabelConfigs:
        - action: labeldrop
          regex: "(app|container|instance|component|namespace)"
#        - action: replace
#          replacement: default
#          target_label: suite

kube-prometheus-stack:
  enabled: false
  namespaceOverride: "monitoring"
  # fullnameOverride: "demo-services"
  alertmanager:
    enabled: false
    alertmanagerSpec:
      logLevel: info
      secrets:
        - alertmanager-secrets
    config:
      global:
        resolve_timeout: 5m
      receivers:
        - name: "null"
        - name: filia_monitoring_slack
          slack_configs:
            - api_url_file: /etc/alertmanager/secrets/alertmanager-secrets/filia-monitoring-slack-url
              channel: "#filia-monitoring"
              send_resolved: true
              text: |-
                {{ range .Alerts }}
                  *Alert:* {{ .Annotations.summary }} - `{{ .Labels.environment }}`
                  *Description:* {{ .Annotations.description }} - `{{ .Labels.severity }}`
                  *Details:*
                  {{ range .Labels.SortedPairs }} â€¢ *{{ .Name }}:* `{{ .Value }}`
                  {{ end }}
                {{ end }}
              title:
                '{{ if eq .Status "firing" }}:fire:{{ else }}:white_check_mark:{{ end
                }}[{{ .Status | toUpper }}{{ if eq .Status "firing" }}{{ .Alerts.Firing
                | len }}{{ end }}] Monitoring Event'
        - name: ops_monitoring_slack
          slack_configs:
            - api_url_file: /etc/alertmanager/secrets/alertmanager-secrets/ops-monitoring-slack-url
              channel: "#ops-monitoring"
              send_resolved: true
              text: |-
                {{ range .Alerts }}
                  *Alert:* {{ .Annotations.summary }} - `{{ .Labels.environment }}`
                  *Description:* {{ .Annotations.description }} - `{{ .Labels.severity }}`
                  *Details:*
                  {{ range .Labels.SortedPairs }}{{ if or (eq .Name "alertname") (eq .Name "cluster") (eq .Name "environment") (eq .Name "severity") (eq .Name "instance") (eq .Name "target") (eq .Name "team") (eq .Name "dashboard") }}*{{ .Name }}*: `{{ .Value }}`
                  {{ end }}{{ end }}
                {{ end }}
              title:
                '{{ if eq .Status "firing" }}:fire:{{ else }}:white_check_mark:{{ end
                }}[{{ .Status | toUpper }}{{ if eq .Status "firing" }}{{ .Alerts.Firing
                | len }}{{ end }}] Monitoring Event'
      route:
        group_by:
          - cluster
          - alertname
        group_interval: 30s
        group_wait: 10s
        receiver: "null"
        repeat_interval: 24h
        routes:
          - match:
              team: development
            receiver: filia_monitoring_slack
          - match:
              team: ops
            receiver: ops_monitoring_slack
  grafana:
    envFromSecret: "loki-secret-auth" # "loki-new-basic-auth"
    enabled: true
    admin:
      existingSecret: "grafana-secret-creds"
    # dashboardProviders:
    #   dashboardproviders.yaml:
    #     apiVersion: 1
    #     providers:
    #       - disableDeletion: false
    #         editable: true
    #         folder: ""
    #         name: filia-dashboards
    #         options:
    #           path: /var/lib/grafana/dashboards/filia-dashboards
    #         orgId: 1
    #         type: file
    # dashboardsConfigMaps:
    #   filia-dashboards: filia-dashboards-configmap
    datasources:
      datasources.yaml:
        apiVersion: 1
        datasources:
          - access: proxy
            isDefault: true
            name: prometheus
            type: prometheus
            url: http://prometheus-kube-prometheus-prometheus.monitoring.svc:9090
        # apiVersion: 1
        # datasources:
        #   - name: Loki
        #     type: loki
        #     url: 'http://prometheus-loki-gateway.monitoring.svc.cluster.local'
        #     access: proxy
        #     basicAuth: true
        #     basicAuthUser: loki # $__env{USERNAME}
        #     secureJsonData:
        #       basicAuthPassword: lokipassword # $__env{PASSWORD}
    grafana.ini:
      auth.anonymous:
        enabled: false
        org_name: Main Org.
        org_role: Viewer
      auth.basic:
        enabled: true
      # dashboards:
      #   default_home_dashboard_path: /var/lib/grafana/dashboards/filia-dashboards/core-system.json
    podAnnotations:
      reloader.stakater.com/auto: "true"
    sidecar:
      dashboards:
        multicluster:
          global:
            enabled: true
      datasources:
        defaultDatasourceEnabled: false
    # ingress:
    #   enabled: true
    #   ingressClassName: public
    #   annotations: {}
    #   hosts:
    #     - xxx
    #   tls:
    #     - secretName: monitoring-tls-secret
    #       hosts:
    #         - xxx
  kubeControllerManager:
    enabled: false
  kubeScheduler:
    enabled: false
  prometheus:
    enabled: false
    service:
      type: ClusterIP
      port: 9090
      targetPort: 9090
      annotations:
        service.beta.kubernetes.io/azure-load-balancer-internal: "true"
        service.beta.kubernetes.io/azure-load-balancer-internal-subnet: "ingress-internal"
    prometheusSpec:
      admissionWebhooks:
        patch:
          enabled: false
      additionalScrapeConfigs:
      - job_name: 'kube-prometheus'
        kubernetes_sd_configs:
          - role: endpoints
        relabel_configs:
          - source_labels: []
            separator: ;
            regex: .*
            target_label: cluster
            replacement: "demo-services"
      - honor_labels: true
        job_name: federate
        metrics_path: /federate
        params:
          match[]:
          - '{nonexistant=""}'
        scrape_interval: 15s
        static_configs:
        - targets:
          - prometheus-kube-prometheus-prometheus.monitoring.svc:9090

#### TCP Services Monitoring
      # - job_name: 'blackbox-tcp-services'
      #   metrics_path: /probe
      #   params:
      #     module: [tcp_connect]
      #   kubernetes_sd_configs:
      #     - role: service
      #   relabel_configs:
      #     # Only keep services with the monitor.tcp=true label
      #     - source_labels: [__meta_kubernetes_service_label_monitor_tcp]
      #       regex: true
      #       action: keep

      #     # Extract service address and inject the correct TCP port
      #     - source_labels: [__address__]
      #       regex: (.*):\d+
      #       replacement: ${1}:4545
      #       target_label: __param_target

      #     # Replace Prometheus's scrape address with Blackbox Exporter service
      #     - target_label: __address__
      #       replacement: prometheus-prometheus-blackbox-exporter.monitoring.svc.cluster.local:9115

      #     # Add labels for filtering/alerts
      #     - source_labels: [__meta_kubernetes_namespace]
      #       target_label: kubernetes_namespace
      #     - source_labels: [__meta_kubernetes_service_name]
      #       target_label: kubernetes_name
      #     - target_label: job
      #       replacement: blackbox-kubernetes-tcp
    - job_name: "blackbox-health/services"
      metrics_path: /probe
      params:
        module: [http_2xx]
      kubernetes_sd_configs:
        - role: service
      relabel_configs:
        # Add "blackbox.io/enabled: blackbox" annotation to enable blackbox monitoring
        - source_labels: [__meta_kubernetes_service_annotation_blackbox_io_enabled]
          action: keep
          regex: true
        - source_labels: [__meta_kubernetes_service_port_number]
          regex: "443"
          action: drop
        # Adds scheme depending on the service port name
        - source_labels: [__meta_kubernetes_service_port_name]
          regex: "(https?)"
          target_label: __scheme__
          replacement: "$1"
        # Overwrite the scheme using blackbox.io/scheme annotation
        - source_labels: [__meta_kubernetes_service_annotation_blackbox_io_scheme]
          regex: "(http|https)"
          target_label: __scheme__
          replacement: "$1"
        # If no annotation path specified default to /api/health
        - source_labels: [__scheme__, __address__]
          regex: "(.+);(.+):(.+)"
          target_label: __param_target
          replacement: "${1}://$2:$3/api/health"
        # Overwrite the path using blackbox.io/path annotation
        - source_labels:
            [
            __scheme__,
            __address__,
            __meta_kubernetes_service_annotation_blackbox_io_path,
            ]
          regex: "(.+);(.+);(.+)"
          replacement: ${1}://${2}${3}
          target_label: __param_target
        # Overwrite the module using blackbox.io/module annotation
        - action: labelmap
          regex: __meta_kubernetes_ingress_annotation_blackbox_io_module
          replacement: __param_module
        - target_label: __address__
          replacement: blackbox-exporter:9115
        - source_labels: [__param_target]
          target_label: instance
        - source_labels: [__meta_kubernetes_namespace]
          target_label: namespace
        - source_labels: [__meta_kubernetes_service_name]
          target_label: service
        # Specify 'Host' header for probe requests using blackbox.io/hostname annotation
        # https://github.com/prometheus/blackbox_exporter#prometheus-configuration
        - source_labels: [__meta_kubernetes_service_annotation_blackbox_io_hostname]
          target_label: __param_hostname
        - source_labels: [__meta_kubernetes_service_annotation_blackbox_io_hostname]
          target_label: hostname


      - job_name: 'blackbox-http-services'
        metrics_path: /probe
        params:
          module: [http_2xx]
        kubernetes_sd_configs:
          - role: service
        relabel_configs:
          - source_labels: [__meta_kubernetes_service_label_monitor_http]
            regex: true
            action: keep
          - source_labels: [__address__]
            regex: (.*):\d+
            replacement: http://${1}:80
            target_label: __param_target
          - target_label: __address__
            replacement: prometheus-prometheus-blackbox-exporter.monitoring.svc.cluster.local:9115
          - source_labels: [__meta_kubernetes_namespace]
            target_label: kubernetes_namespace
          - source_labels: [__meta_kubernetes_service_name]
            target_label: kubernetes_name
      portName: http
      resources:
        limits:
          memory: "2500Mi"
        requests:
          memory: "400Mi"
      externalLabels:
        cluster: "default"
      ruleSelector:
        matchLabels: {}
      serviceMonitorSelector: {}
      storageSpec:
        volumeClaimTemplate:
          spec:
            accessModes: ["ReadWriteOnce"]
            resources:
              requests:
                storage: "16Gi"
  prometheus-node-exporter:
    {}
    # resources:
    #   limits:
    #     cpu: 200m
    #     memory: 100Mi
  prometheusOperator:
    resources:
      limits:
        cpu: 200m
        memory: 200Mi
      requests:
        cpu: 100m
        memory: 100Mi

loki:
#  nodeSelector: &nodeSelector
#    role: "loggin"
##    nodeSelector: "minikube-vcluster"
#
#  commonAffinity: &commonAffinity
#    nodeAffinity:
#      requiredDuringSchedulingIgnoredDuringExecution:
#        nodeSelectorTerms:
#          - matchExpressions:
#              - key: "role"
#                operator: Equals
#                values:
#                  - logging
#    podAntiAffinity: {}

#  commonTolerations: &commonTolerations
#    - key: "logging"
#      operator: "Equal"
#      value: "true"
#      effect: "NoSchedule"

#  chunksCache:
#    nodeSelector: *nodeSelector
#    affinity: *commonAffinity
#    tolerations: *commonTolerations

  singleBinary:
    replicas: 1
    persistence:
      enabled: true
      storageClassName: standard  # or "local-path" or any StorageClass in your cluster
      accessModes:
        - ReadWriteOnce
      size: 10Gi
  deploymentMode: SingleBinary
  lokiCanary:
    enabled: false
  test:
    enabled: false
  backend:
    enabled: false
    replicas: 0
  read:
    enabled: false
    replicas: 0
  write:
    enabled: false
    replicas: 0

  loki:
    auth_enabled: false
    commonConfig:
      replication_factor: 1
      ring:
        kvstore:
          store: memberlist

    compactor:
      retention_enabled: false
#      compaction_interval: 10m
#      retention_delete_delay: 2h
#      retention_delete_worker_count: 150
#      working_directory: /var/loki/data/retention

    ingester:
      chunk_encoding: snappy

    pattern_ingester:
      enabled: true

    limits_config:
      allow_structured_metadata: true
      volume_enabled: true
      retention_period: 672h

    schemaConfig:
      configs:
        - from: "2023-07-01"
          store: boltdb-shipper
          object_store: filesystem
          schema: v12
          index:
            period: 24h
            prefix: index_
          chunks:
            prefix: chunk_
            period: 24h
        - from: "2024-05-07"
          store: tsdb
          object_store: filesystem
          schema: v13
          index:
            period: 24h
            prefix: index_

    storage_config:
      tsdb_shipper:
        active_index_directory: /var/loki/data/loki/index
        cache_location: /var/loki/data/loki/boltdb-cache
        cache_ttl: "24h"
      filesystem:
        directory: /var/loki/data/loki/chunks

    storage:
      type: filesystem
      filesystem:
        chunks_directory: /var/loki/data/loki/chunks

    podSecurityContext:
      fsGroup: 10001
      runAsGroup: 10001
      runAsNonRoot: true
      runAsUser: 10001

    monitoring:
      serviceMonitor:
        enabled: true
      selfMonitoring:
        enabled: false
        grafanaAgent:
          installOperator: false
      lokiCanary:
        enabled: false

    rbac:
      pspEnabled: false

    querier:
      max_concurrent: 4

  tableManager:
    retention_deletes_enabled: "false"
    retention_period: "336h"

  gateway:
    basicAuth:
      enabled: true
      existingSecret: loki-basic-auth-htpasswd # loki-secret-creds
#    affinity: *commonAffinity
#      tolerations: *commonTolerations
#  serviceAccount:
#    name: loki # The app ID of the Azure AD app
#    labels:
#      "azure.workload.identity/use": "true"



prometheus-blackbox-exporter:
  enabled: false
  namespaceOverride: "monitoring"
  tolerations:
  - key: "logging"
    operator: "Equal"
    value: "true"
    effect: "NoSchedule"
  config:
    modules:
      tcp_prober:
        prober: tcp
        tcp:
          ip_protocol_fallback: false
          preferred_ip_protocol: ip4
      http_endpoint:
        prober: http
        timeout: 5s
        http:
          valid_http_versions: [ "HTTP/1.1", "HTTP/2.0" ]
          valid_status_codes: [ 200, 204 ]
          no_follow_redirects: false
          preferred_ip_protocol: "ip4"
  serviceMonitor:
    enabled: true
    defaults:
      labels:
        release: prometheus
  prometheusRule:
    enabled: true
    additionalLabels:
      release: prometheus